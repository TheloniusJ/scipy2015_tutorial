{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1789)\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Data Modeling\n",
    "\n",
    "Pandas, NumPy and SciPy provide the core functionality for building statistical models of our data. We use models to:\n",
    "\n",
    "- Concisely **describe** the components of our data\n",
    "- Provide **inference** about underlying parameters that may have generated the data\n",
    "- Make **predictions** about unobserved data, or expected future observations.\n",
    "\n",
    "This section of the tutorial illustrates how to use Python to build statistical models of low to moderate difficulty from scratch, and use them to extract estimates and associated measures of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation\n",
    "==========\n",
    "\n",
    "An recurring statistical problem is finding estimates of the relevant parameters that correspond to the distribution that best represents our data.\n",
    "\n",
    "In **parametric** inference, we specify *a priori* a suitable distribution, then choose the parameters that best fit the data.\n",
    "\n",
    "* e.g. the mean $\\mu$ and the variance $\\sigma^2$ in the case of the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([ 1.00201077,  1.58251956,  0.94515919,  6.48778002,  1.47764604,\n",
    "        5.18847071,  4.21988095,  2.85971522,  3.40044437,  3.74907745,\n",
    "        1.18065796,  3.74748775,  3.27328568,  3.19374927,  8.0726155 ,\n",
    "        0.90326139,  2.34460034,  2.14199217,  3.27446744,  3.58872357,\n",
    "        1.20611533,  2.16594393,  5.56610242,  4.66479977,  2.3573932 ])\n",
    "_ = plt.hist(x, bins=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data to probability distributions\n",
    "\n",
    "We start with the problem of finding values for the parameters that provide the best fit between the model and the data, called point estimates. First, we need to define what we mean by ‘best fit’. There are two commonly used criteria:\n",
    "\n",
    "* **Method of moments** chooses the parameters so that the sample moments (typically the sample mean and variance) match the theoretical moments of our chosen distribution.\n",
    "* **Maximum likelihood** chooses the parameters to maximize the likelihood, which measures how likely it is to observe our given sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Random Variables\n",
    "\n",
    "$$X = \\{0,1\\}$$\n",
    "\n",
    "$$Y = \\{\\ldots,-2,-1,0,1,2,\\ldots\\}$$\n",
    "\n",
    "**Probability Mass Function**: \n",
    "\n",
    "For discrete $X$,\n",
    "\n",
    "$$Pr(X=x) = f(x|\\theta)$$\n",
    "\n",
    "![Discrete variable](http://upload.wikimedia.org/wikipedia/commons/1/16/Poisson_pmf.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***e.g. Poisson distribution***\n",
    "\n",
    "The Poisson distribution models unbounded counts:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$Pr(X=x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}$$\n",
    "</div>\n",
    "\n",
    "* $X=\\{0,1,2,\\ldots\\}$\n",
    "* $\\lambda > 0$\n",
    "\n",
    "$$E(X) = \\text{Var}(X) = \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Random Variables\n",
    "\n",
    "$$X \\in [0,1]$$\n",
    "\n",
    "$$Y \\in (-\\infty, \\infty)$$\n",
    "\n",
    "**Probability Density Function**: \n",
    "\n",
    "For continuous $X$,\n",
    "\n",
    "$$Pr(x \\le X \\le x + dx) = f(x|\\theta)dx \\, \\text{ as } \\, dx \\rightarrow 0$$\n",
    "\n",
    "![Continuous variable](http://upload.wikimedia.org/wikipedia/commons/e/ec/Exponential_pdf.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***e.g. normal distribution***\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]$$\n",
    "</div>\n",
    "\n",
    "* $X \\in \\mathbf{R}$\n",
    "* $\\mu \\in \\mathbf{R}$\n",
    "* $\\sigma>0$\n",
    "\n",
    "$$\\begin{align}E(X) &= \\mu \\cr\n",
    "\\text{Var}(X) &= \\sigma^2 \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Nashville Precipitation\n",
    "\n",
    "The dataset `nashville_precip.txt` contains [NOAA precipitation data for Nashville measured since 1871](http://bit.ly/nasvhville_precip_data). \n",
    "\n",
    "![rain](images/main.jpg)\n",
    "\n",
    "The gamma distribution is often a good fit to aggregated rainfall data, and will be our candidate distribution in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip = pd.read_table(\"../data/nashville_precip.txt\", index_col=0, na_values='NA', delim_whitespace=True)\n",
    "precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = precip.hist(sharex=True, sharey=True, grid=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is recognizing what sort of distribution to fit our data to. A couple of observations:\n",
    "\n",
    "1. The data are skewed, with a longer tail to the right than to the left\n",
    "2. The data are positive-valued, since they are measuring rainfall\n",
    "3. The data are continuous\n",
    "\n",
    "There are a few possible choices, but one suitable alternative is the **gamma distribution**:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$x \\sim \\text{Gamma}(\\alpha, \\beta) = \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-\\beta x}}{\\Gamma(\\alpha)}$$\n",
    "</div>\n",
    "\n",
    "![gamma](http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Gamma_distribution_pdf.svg/500px-Gamma_distribution_pdf.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***method of moments*** simply assigns the empirical mean and variance to their theoretical counterparts, so that we can solve for the parameters.\n",
    "\n",
    "So, for the gamma distribution, the mean and variance are:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$ \\hat{\\mu} = \\bar{X} = \\alpha \\beta $$\n",
    "$$ \\hat{\\sigma}^2 = S^2 = \\alpha \\beta^2 $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we solve for these parameters, we can use a gamma distribution to describe our data:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$ \\alpha = \\frac{\\bar{X}^2}{S^2}, \\, \\beta = \\frac{S^2}{\\bar{X}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with the missing value in the October data. Given what we are trying to do, it is most sensible to fill in the missing value with the average of the available values. We will learn more sophisticated methods for handling missing data later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip.fillna(value={'Oct': precip.Oct.mean()}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the sample moments of interest, the means and variances by month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip_mean = precip.mean()\n",
    "precip_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precip_var = precip.var()\n",
    "precip_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use these moments to estimate $\\alpha$ and $\\beta$ for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_mom = precip_mean ** 2 / precip_var\n",
    "beta_mom = precip_var / precip_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_mom, beta_mom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `gamma.pdf` function in `scipy.stats.distributions` to plot the ditribtuions implied by the calculated alphas and betas. For example, here is January:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import gamma\n",
    "\n",
    "precip.Jan.hist(normed=True, bins=20)\n",
    "plt.plot(np.linspace(0, 10), gamma.pdf(np.linspace(0, 10), alpha_mom[0], beta_mom[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping over all months, we can create a grid of plots for the distribution of rainfall, using the gamma distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axs = precip.hist(normed=True, figsize=(12, 8), sharex=True, sharey=True, bins=15, grid=False)\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    \n",
    "    # Get month\n",
    "    m = ax.get_title()\n",
    "    \n",
    "    # Plot fitted distribution\n",
    "    x = np.linspace(*ax.get_xlim())\n",
    "    ax.plot(x, gamma.pdf(x, alpha_mom[m], beta_mom[m]))\n",
    "    \n",
    "    # Annotate with parameter estimates\n",
    "    label = 'alpha = {0:.2f}\\nbeta = {1:.2f}'.format(alpha_mom[m], beta_mom[m])\n",
    "    ax.annotate(label, xy=(10, 0.2))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood\n",
    "==================\n",
    "\n",
    "**Maximum likelihood** (ML) fitting is usually more work than the method of moments, but it is preferred as the resulting estimator is known to have good theoretical properties. \n",
    "\n",
    "There is a ton of theory regarding ML. We will restrict ourselves to the mechanics here.\n",
    "\n",
    "Say we have some data $y = y_1,y_2,\\ldots,y_n$ that is distributed according to some distribution:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$Pr(Y_i=y_i | \\theta)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for example, is a **Poisson distribution** that describes the distribution of some discrete variables, typically *counts*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.random.poisson(5, size=100)\n",
    "plt.hist(y, bins=12, normed=True)\n",
    "plt.xlabel('y'); plt.ylabel('Pr(y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product $\\prod_{i=1}^n Pr(y_i | \\theta)$ gives us a measure of how **likely** it is to observe values $y_1,\\ldots,y_n$ given the parameters $\\theta$. \n",
    "\n",
    "Maximum likelihood fitting consists of choosing the appropriate function $l= Pr(Y|\\theta)$ to maximize for a given set of observations. We call this function the *likelihood function*, because it is a measure of how likely the observations are if the model is true.\n",
    "\n",
    "> Given these data, how likely is this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model, the data were drawn from a Poisson distribution with parameter $\\lambda =5$.\n",
    "\n",
    "$$L(y|\\lambda=5) = \\frac{e^{-5} 5^y}{y!}$$\n",
    "\n",
    "So, for any given value of $y$, we can calculate its likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poisson_like = lambda x, lam: np.exp(-lam) * (lam**x) / (np.arange(x)+1).prod()\n",
    "\n",
    "lam = 6\n",
    "value = 10\n",
    "poisson_like(value, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(poisson_like(yi, lam) for yi in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lam = 8\n",
    "np.sum(poisson_like(yi, lam) for yi in y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the likelihood function for any value of the parameter(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,15)\n",
    "x = 5\n",
    "plt.plot(lambdas, [poisson_like(x, l) for l in lambdas])\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('L($\\lambda$|x={0})'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the likelihood function different than the probability distribution function (PDF)? The likelihood is a function of the parameter(s) *given the data*, whereas the PDF returns the probability of data given a particular parameter value. Here is the PDF of the Poisson for $\\lambda=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lam = 5\n",
    "xvals = np.arange(15)\n",
    "plt.bar(xvals, [poisson_like(x, lam) for x in xvals], width=0.2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Pr(X|$\\lambda$=5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why are we interested in the likelihood function?*\n",
    "\n",
    "A reasonable estimate of the true, unknown value for the parameter is one which **maximizes the likelihood function**. So, inference is reduced to an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the rainfall data, if we are using a gamma distribution we need to maximize:\n",
    "\n",
    "$$\\begin{align}l(\\alpha,\\beta) &= \\sum_{i=1}^n \\log[\\beta^{\\alpha} x^{\\alpha-1} e^{-x/\\beta}\\Gamma(\\alpha)^{-1}] \\cr \n",
    "&= n[(\\alpha-1)\\overline{\\log(x)} - \\bar{x}\\beta + \\alpha\\log(\\beta) - \\log\\Gamma(\\alpha)]\\end{align}$$\n",
    "\n",
    "*N.B.: Its usually easier to work in the log scale*\n",
    "\n",
    "where $n = 2012 − 1871 = 141$ and the bar indicates an average over all *i*. We choose $\\alpha$ and $\\beta$ to maximize $l(\\alpha,\\beta)$.\n",
    "\n",
    "Notice $l$ is infinite if any $x$ is zero. We do not have any zeros, but we do have an NA value for one of the October data, which we dealt with above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the MLE\n",
    "\n",
    "To find the maximum of any function, we typically take the *derivative* with respect to the variable to be maximized, set it to zero and solve for that variable. \n",
    "\n",
    "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\beta} = n\\left(\\frac{\\alpha}{\\beta} - \\bar{x}\\right) = 0$$\n",
    "\n",
    "Which can be solved as $\\beta = \\alpha/\\bar{x}$. However, plugging this into the derivative with respect to $\\alpha$ yields:\n",
    "\n",
    "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\alpha} = \\log(\\alpha) + \\overline{\\log(x)} - \\log(\\bar{x}) - \\frac{\\Gamma(\\alpha)'}{\\Gamma(\\alpha)} = 0$$\n",
    "\n",
    "This has no closed form solution. We must use ***numerical optimization***!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical optimization alogarithms take an initial \"guess\" at the solution, and **iteratively** improve the guess until it gets \"close enough\" to the answer.\n",
    "\n",
    "Here, we will use *Newton-Raphson* method, which is a **root-finding algorithm**:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is available to us via SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a graphical example of how Newton-Raphson converges on a solution, using an arbitrary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run newton_raphson_plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the Newton-Raphson algorithm, we need a function that returns a vector containing the **first and second derivatives** of the function with respect to the variable of interest. The second derivative of the gamma distribution with respect to $\\alpha$ is:\n",
    "\n",
    "$$\\frac{\\partial^2 l(\\alpha,\\beta)}{\\partial \\alpha^2} = \\frac{1}{\\alpha} - \\frac{\\partial}{\\partial \\alpha} \\left[ \\frac{\\Gamma(\\alpha)'}{\\Gamma(\\alpha)} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import psi, polygamma\n",
    "\n",
    "dlgamma = lambda a, log_mean, mean_log: np.log(a) - psi(a) - log_mean + mean_log\n",
    "dl2gamma = lambda a, *args: 1./a - polygamma(1, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `log_mean` and `mean_log` are $\\log{\\bar{x}}$ and $\\overline{\\log(x)}$, respectively. `psi` and `polygamma` are complex functions of the Gamma function that result when you take first and second derivatives of that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "log_mean = precip.mean().apply(np.log)\n",
    "mean_log = precip.apply(np.log).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to optimize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alpha MLE for December\n",
    "alpha_mle = newton(dlgamma, 2, dl2gamma, args=(log_mean[-1], mean_log[-1]))\n",
    "alpha_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plug this back into the solution for beta:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\beta  = \\frac{\\alpha}{\\bar{X}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_mle = alpha_mle/precip.mean()[-1]\n",
    "beta_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the fit of the estimates derived from MLE to those from the method of moments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dec = precip.Dec\n",
    "dec.hist(normed=True, bins=10, grid=False)\n",
    "x = np.linspace(0, dec.max())\n",
    "plt.plot(x, gamma.pdf(x, alpha_mom[-1], beta_mom[-1]), 'm-', label='Moment estimator')\n",
    "plt.plot(x, gamma.pdf(x, alpha_mle, beta_mle), 'r--', label='ML estimator')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some common distributions, SciPy includes methods for fitting via MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "\n",
    "gamma.fit(precip.Dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit is not directly comparable to our estimates, however, because SciPy's `gamma.fit` method fits an odd 3-parameter version of the gamma distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model checking\n",
    "\n",
    "An informal way of checking the fit of our parametric model is to compare the observed quantiles of the data to those of the theoretical model we are fitting it to. If the model is a good fit, the points should fall on a 45-degree reference line. This is called a **probability plot**.\n",
    "\n",
    "SciPy includes a `probplot` function that generates probability plots based on the data and a specified distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import probplot\n",
    "\n",
    "probplot(precip.Dec, dist=gamma(3.51, scale=0.84), plot=plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: truncated distribution\n",
    "\n",
    "Suppose that we observe $Y$ truncated below at $a$ (where $a$ is known). If $X$ is the distribution of our observation, then:\n",
    "\n",
    "$$ P(X \\le x) = P(Y \\le x|Y \\gt a) = \\frac{P(a \\lt Y \\le x)}{P(Y \\gt a)}$$\n",
    "\n",
    "(so, $Y$ is the original variable and $X$ is the truncated variable) \n",
    "\n",
    "Then X has the density:\n",
    "\n",
    "$$f_X(x) = \\frac{f_Y (x)}{1−F_Y (a)} \\, \\text{for} \\, x \\gt a$$ \n",
    "\n",
    "Suppose $Y \\sim N(\\mu, \\sigma^2)$ and $x_1,\\ldots,x_n$ are independent observations of $X$. We can use maximum likelihood to find $\\mu$ and $\\sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can simulate a truncated distribution using a `while` statement to eliminate samples that are outside the support of the truncated distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(size=10000)\n",
    "\n",
    "# Truncation point\n",
    "a = -1\n",
    "\n",
    "# Resample until all points meet criterion\n",
    "x_small = x < a\n",
    "while x_small.sum():\n",
    "    x[x_small] = np.random.normal(size=x_small.sum())\n",
    "    x_small = x < a\n",
    "    \n",
    "_ = plt.hist(x, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a log likelihood for this function using the conditional form:\n",
    "\n",
    "$$f_X(x) = \\frac{f_Y (x)}{1−F_Y (a)} \\, \\text{for} \\, x \\gt a$$ \n",
    "\n",
    "The denominator normalizes the truncated distribution so that it integrates to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import norm\n",
    "\n",
    "trunc_norm = lambda theta, a, x: -(np.log(norm.pdf(x, theta[0], theta[1])) - \n",
    "                                      np.log(1 - norm.cdf(a, theta[0], theta[1]))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use an optimization algorithm, the **Nelder-Mead simplex algorithm**. It has a couple of advantages: \n",
    "\n",
    "- it does not require derivatives\n",
    "- it can optimize (minimize) a vector of parameters\n",
    "\n",
    "SciPy implements this algorithm in its `fmin` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "fmin(trunc_norm, np.array([1,2]), args=(-1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, simulating data is a terrific way of testing your model before using it with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimates\n",
    "\n",
    "In some instances, we may not be interested in the parameters of a particular distribution of data, but just a smoothed representation of the data at hand. In this case, we can estimate the disribution *non-parametrically* (i.e. making no assumptions about the form of the underlying distribution) using kernel density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some random data\n",
    "y = np.random.normal(10, size=15)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel estimator is a sum of symmetric densities centered at each observation. The selected kernel function determines the shape of each component while the **bandwidth** determines their spread. For example, if we use a Gaussian kernel function, the variance acts as the bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(7, 13, 100)\n",
    "# Smoothing parameter\n",
    "s = 0.3\n",
    "# Calculate the kernels\n",
    "kernels = np.transpose([norm.pdf(x, yi, s) for yi in y])\n",
    "plt.plot(x, kernels, 'k:')\n",
    "plt.plot(x, kernels.sum(1))\n",
    "plt.plot(y, np.zeros(len(y)), 'ro', ms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy implements a Gaussian KDE that automatically chooses an appropriate bandwidth. Let's create a bi-modal distribution of data that is not easily summarized by a parametric distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a bi-modal distribution with a mixture of Normals.\n",
    "x1 = np.random.normal(0, 2, 50)\n",
    "x2 = np.random.normal(5, 1, 50)\n",
    "\n",
    "# Append by row\n",
    "x = np.r_[x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(x, bins=10, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kde\n",
    "\n",
    "density = kde.gaussian_kde(x)\n",
    "xgrid = np.linspace(x.min(), x.max(), 100)\n",
    "plt.hist(x, bins=8, normed=True)\n",
    "plt.plot(xgrid, density(xgrid), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Comparative Chopstick Effectiveness\n",
    "\n",
    "A few researchers set out to determine what the optimal length for chopsticks is. The dataset `chopstick-effectiveness.csv` includes measurements of \"Food Pinching Efficiency\" across a range of chopstick lengths for 31 individuals.\n",
    "\n",
    "Use the method of moments or MLE to calculate the mean and variance of food pinching efficiency for each chopstick length. This means you need to select an appropriate distributional form for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
